{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a44ab11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Year range 2002-2004, bin width = 3\n",
      "  • K-Means (k=3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "k_means requires at least two arguments: (data, k)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 97\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  • \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_label\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# run & time & cluster & score via your helper\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m result, runtime, n_clusters, pts_per_clust, metrics \u001b[38;5;241m=\u001b[39m run_clustering_dbcv_score(\n\u001b[0;32m     98\u001b[0m     model_fn,\n\u001b[0;32m     99\u001b[0m     run_label,\n\u001b[0;32m    100\u001b[0m     X_norm,\n\u001b[0;32m    101\u001b[0m     X_min,\n\u001b[0;32m    102\u001b[0m     X_max,\n\u001b[0;32m    103\u001b[0m     plot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    105\u001b[0m )\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# optional: round details if present\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(metrics) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\fionu\\OneDrive\\Dokumente\\Daten Fiona\\AIUB\\Code\\AIUB-Backup\\clustering\\cluster_data.py:97\u001b[0m, in \u001b[0;36mrun_clustering_dbcv_score\u001b[1;34m(algorithm, name, data, data_min, data_max, *args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m plot \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplot\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Default to True if not provided\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Start the clustering algorithm and calculate runtime\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m result, runtime \u001b[38;5;241m=\u001b[39m estimate_runtime(algorithm, data, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     99\u001b[0m n_clusters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(result\u001b[38;5;241m.\u001b[39mlabels))  \u001b[38;5;66;03m# Number of unique labels (clusters)\u001b[39;00m\n\u001b[0;32m    100\u001b[0m points_per_cluster \u001b[38;5;241m=\u001b[39m {i: \u001b[38;5;28mlist\u001b[39m(result\u001b[38;5;241m.\u001b[39mlabels)\u001b[38;5;241m.\u001b[39mcount(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(result\u001b[38;5;241m.\u001b[39mlabels)}  \u001b[38;5;66;03m# Count points per cluster\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fionu\\OneDrive\\Dokumente\\Daten Fiona\\AIUB\\Code\\AIUB-Backup\\clustering\\cluster_data.py:354\u001b[0m, in \u001b[0;36mestimate_runtime\u001b[1;34m(clustering_func, build_function, swap_function, *args, **kwargs)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clustering_func \u001b[38;5;129;01min\u001b[39;00m k_based_methods:\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m--> 354\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclustering_func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires at least two arguments: (data, k)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    355\u001b[0m     data, k \u001b[38;5;241m=\u001b[39m args[:\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;66;03m# Ensure k is an integer\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: k_means requires at least two arguments: (data, k)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# clustering backends\n",
    "from kmeans import k_means\n",
    "from DBSCAN import dbscan_clustering\n",
    "from HDBSCAN import hdbscan_clustering\n",
    "from OPTICS import optics_clustering\n",
    "\n",
    "# your utilities\n",
    "import cluster_data\n",
    "from cluster_data import (\n",
    "    run_clustering_dbcv_score,\n",
    "    normalize_data,\n",
    "    unnormalize,\n",
    "    generate_running_year_ranges,\n",
    "    bin_data_for_clustering\n",
    ")\n",
    "from cluster_plotter import ClusterPlotter\n",
    "import cluster_plotter\n",
    "import scores\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Define your algorithms + parameter grids once\n",
    "# ------------------------------------------------------------------------\n",
    "algorithms = {\n",
    "    'K-Means': {\n",
    "        'model': k_means,\n",
    "        'params': [{'k': 3}, {'k': 4}, {'k': 5}]\n",
    "    },\n",
    "    'DBSCAN': {\n",
    "        'model': dbscan_clustering,\n",
    "        'params': [\n",
    "            {'eps': 0.05, 'min_samples': 25},\n",
    "            {'eps': 0.01, 'min_samples': 30},\n",
    "            {'eps': 0.02, 'min_samples': 35},\n",
    "        ]\n",
    "    },\n",
    "    'HDBSCAN': {\n",
    "        'model': hdbscan_clustering,\n",
    "        'params': [\n",
    "            {'min_samples': 5, 'min_cluster_size': 15},\n",
    "            {'min_samples': 10, 'min_cluster_size': 20},\n",
    "            {'min_samples': 15, 'min_cluster_size': 25},\n",
    "        ]\n",
    "    },\n",
    "    'OPTICS': {\n",
    "        'model': optics_clustering,\n",
    "        'params': [\n",
    "            {'min_samples': 50, 'max_eps': 100, 'xi': 0.005},\n",
    "            {'min_samples': 100, 'max_eps': 200, 'xi': 0.002},\n",
    "            {'min_samples': 150, 'max_eps': 500, 'xi': 0.001},\n",
    "        ]\n",
    "    },\n",
    "}\n",
    "\n",
    "# bin widths to try\n",
    "bins = [3, 4, 5, 6]\n",
    "\n",
    "# where to save everything\n",
    "plot_dir = \"Images/binning_width_all_algorithms\"\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "cluster_plotter.clear_directory(plot_dir)\n",
    "\n",
    "# collect for later multi‐plot\n",
    "array_of_metrics    = []\n",
    "array_of_yearranges = []\n",
    "array_of_binwidths  = []\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Outer loop: vary your running‐window width b\n",
    "# ------------------------------------------------------------------------\n",
    "for b in bins:\n",
    "    running_ranges = generate_running_year_ranges(2002, 2023, b)\n",
    "    binned_data    = bin_data_for_clustering(running_ranges, print_res=False)\n",
    "\n",
    "    # Inner loop: each binned chunk\n",
    "    for data_chunk, year_range in binned_data:\n",
    "        print(f\"\\n=== Year range {year_range}, bin width = {b}\")\n",
    "\n",
    "        # prepare & normalize\n",
    "        X = np.vstack((data_chunk.inc, data_chunk.raan)).T\n",
    "        X_norm, X_min, X_max = normalize_data(X)\n",
    "\n",
    "        # for each algorithm & its param‐grid\n",
    "        for algo_name, algo_info in algorithms.items():\n",
    "            model_fn = algo_info['model']\n",
    "\n",
    "            for params in algo_info['params']:\n",
    "                # build a label to show in plot titles, etc.\n",
    "                param_str = \", \".join(f\"{k}={v}\" for k, v in params.items())\n",
    "                run_label = f\"{algo_name} ({param_str})\"\n",
    "                print(f\"  • {run_label}\")\n",
    "\n",
    "                # run & time & cluster & score via your helper\n",
    "                result, runtime, n_clusters, pts_per_clust, metrics = run_clustering_dbcv_score(\n",
    "                    model_fn,\n",
    "                    run_label,\n",
    "                    X_norm,\n",
    "                    X_min,\n",
    "                    X_max,\n",
    "                    plot=False,\n",
    "                    **params\n",
    "                )\n",
    "\n",
    "                # optional: round details if present\n",
    "                if len(metrics) >= 7:\n",
    "                    stds   = {k: tuple(round(v,3) for v in vals) for k, vals in metrics[4].items()}\n",
    "                    sqdens = {k: round(v,3)                    for k, v    in metrics[5].items()}\n",
    "                    hulld  = {k: round(v,3)                    for k, v    in metrics[6].items()}\n",
    "                else:\n",
    "                    stds = sqdens = hulld = None\n",
    "\n",
    "                # collect for later\n",
    "                array_of_metrics.append(metrics[:4])\n",
    "                array_of_yearranges.append(year_range)\n",
    "                array_of_binwidths.append(b)\n",
    "\n",
    "                # plot each clustering in un-normalized space\n",
    "                X_unnorm, centers = unnormalize(\n",
    "                    result.data,\n",
    "                    result.cluster_centers,\n",
    "                    X_min,\n",
    "                    X_max\n",
    "                )\n",
    "                plotter = ClusterPlotter(X_unnorm, result.labels, centers)\n",
    "                fname   = f\"{algo_name}_{year_range}_{b}.png\"\n",
    "                title   = f\"{run_label} — years {year_range}, bin={b}\"\n",
    "                plotter.clusters_2d_plot(title, os.path.join(plot_dir, fname))\n",
    "\n",
    "# once all done, show how your score (e.g. DBCV) varies\n",
    "scores.plot_scores_for_different_binnings(\n",
    "    array_of_metrics,\n",
    "    array_of_yearranges,\n",
    "    array_of_binwidths,\n",
    "    plot_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09899f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running clustering for Year Range: 2002-2005\n",
      "\n",
      "Running K-Means with parameters {'n_clusters': 3}\n",
      "Calculating DBCV score for K-Means with parameters {'n_clusters': 3}\n",
      "-0.9539274486321931\n",
      "\n",
      "Running K-Means with parameters {'n_clusters': 4}\n",
      "Calculating DBCV score for K-Means with parameters {'n_clusters': 4}\n",
      "-0.9452196175603331\n",
      "\n",
      "Running K-Means with parameters {'n_clusters': 5}\n",
      "Calculating DBCV score for K-Means with parameters {'n_clusters': 5}\n",
      "-0.9367540828320728\n",
      "\n",
      "Running DBSCAN with parameters {'eps': 0.05, 'min_samples': 25}\n",
      "Calculating DBCV score for DBSCAN with parameters {'eps': 0.05, 'min_samples': 25}\n",
      "-0.794687843346411\n",
      "\n",
      "Running DBSCAN with parameters {'eps': 0.01, 'min_samples': 30}\n",
      "Calculating DBCV score for DBSCAN with parameters {'eps': 0.01, 'min_samples': 30}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from OPTICS import optics_clustering\n",
    "from kmeans import k_means\n",
    "from HDBSCAN import hdbscan_clustering\n",
    "from DBSCAN import dbscan_clustering\n",
    "\n",
    "from cluster_data import normalize_data, unnormalize\n",
    "from cluster_plotter import ClusterPlotter\n",
    "from cluster_data import run_clustering\n",
    "import cluster_data\n",
    "import scores\n",
    "import cluster_plotter\n",
    "\n",
    "\n",
    "# Files for uncorr_observed_data\n",
    "uncorr_obs_files = {\n",
    "    year: f\"ogs{year}01_12_det.ele_ucorr\" if year != 2002 else f\"ogs{year}08_12_det.ele_ucorr\"\n",
    "    for year in range(2002, 2024) if year != 2018  # Exclude 2018, missing\n",
    "}\n",
    "\n",
    "# Standard year ranges\n",
    "standard_year_ranges = {f\"{start}-{start + 3}\": np.arange(start, start + 4) for start in [2002, 2006, 2010, 2014]}\n",
    "standard_year_ranges[\"2019-2023\"] = np.arange(2019, 2024)\n",
    "\n",
    "# Running ranges\n",
    "running_ranges = cluster_data.generate_running_year_ranges(2002, 2023, 4)\n",
    "year_ranges = standard_year_ranges\n",
    "\n",
    "# Bin observed data\n",
    "binned_data = cluster_data.bin_observed_data(uncorr_obs_files, year_ranges, print_res=False)\n",
    "results_per_year_range = {}\n",
    "\n",
    "# Directory for saving plots\n",
    "plot_dir = \"Images/comparison_algorithms_dbcv\"\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "cluster_plotter.clear_directory(plot_dir)\n",
    "\n",
    "# Define score columns (needed for correlation matrices)\n",
    "score_columns = [\"DBCV Score\"]\n",
    "\n",
    "# Process each binned data set\n",
    "for cluster_data_single, year_range in binned_data:\n",
    "    print(f\"\\nRunning clustering for Year Range: {year_range}\")\n",
    "\n",
    "    # Prepare data array\n",
    "    data_array = np.array([cluster_data_single.inc, cluster_data_single.raan]).T\n",
    "    normalized_data, data_min, data_max = normalize_data(data_array)\n",
    "\n",
    "    # Algorithms and parameters\n",
    "    algorithms = {\n",
    "        'K-Means': {'model': k_means, 'params': [{'n_clusters': 3}, {'n_clusters': 4}, {'n_clusters': 5}]},\n",
    "        'DBSCAN': {'model': dbscan_clustering, 'params': [{'eps': 0.05, 'min_samples': 25}, {'eps': 0.01, 'min_samples': 30}, {'eps': 0.02, 'min_samples': 35}]},\n",
    "        'HDBSCAN': {'model': hdbscan_clustering, 'params': [{'min_samples': 5, 'min_cluster_size': 15}, {'min_samples': 10, 'min_cluster_size': 20}, {'min_samples': 15, 'min_cluster_size': 25}]},\n",
    "        'OPTICS': {'model': optics_clustering, 'params': [{'min_samples': 50, 'max_eps': 100, 'xi': 0.005}, {'min_samples': 100, 'max_eps': 200, 'xi': 0.002}, {'min_samples': 150, 'max_eps': 500, 'xi': 0.001}]}\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    dbcv_scores = []\n",
    "    \n",
    "    global_min = -1\n",
    "    global_max = 1\n",
    "\n",
    "    for algo_name, algo_info in algorithms.items():\n",
    "        for params in algo_info['params']:\n",
    "            print(f\"\\nRunning {algo_name} with parameters {params}\")\n",
    "\n",
    "            if algo_name == \"K-Means\":\n",
    "                model = k_means(normalized_data, k=params['n_clusters'])\n",
    "                labels = model.labels\n",
    "                n_clusters = len(set(labels))\n",
    "\n",
    "            elif algo_name == \"DBSCAN\":\n",
    "                model = dbscan_clustering(normalized_data, eps=params['eps'], min_samples=params['min_samples'])\n",
    "                labels = model.labels\n",
    "                n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "            elif algo_name == \"HDBSCAN\":\n",
    "                model = hdbscan_clustering(normalized_data, min_cluster_size=params['min_cluster_size'], min_samples=params['min_samples'])\n",
    "                labels = model.labels\n",
    "                n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "            elif algo_name == \"OPTICS\":\n",
    "                model = optics_clustering(normalized_data, min_samples=params['min_samples'], max_eps=params['max_eps'], xi=params['xi'])\n",
    "                labels = model.labels\n",
    "                n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "            if n_clusters > 1:\n",
    "                try:\n",
    "                    print(f\"Calculating DBCV score for {algo_name} with parameters {params}\")\n",
    "                    dbcv_score = scores.DBCV_score(model)\n",
    "                    print(dbcv_score)\n",
    "                    dbcv_scores.append(dbcv_score)\n",
    "\n",
    "                    results.append({\n",
    "                        \"Algorithm\": algo_name,\n",
    "                        \"Parameters\": params,\n",
    "                        \"DBCV Score\": dbcv_score,\n",
    "                        \"Clusters\": n_clusters\n",
    "                    })\n",
    "\n",
    "                except ValueError as e:\n",
    "                    print(f\"Error calculating scores: {e}\")\n",
    "                    pass\n",
    "\n",
    "    # Updating the global min/max values\n",
    "    if dbcv_scores: \n",
    "        global_min = np.min(dbcv_scores)\n",
    "        global_max = np.max(dbcv_scores)\n",
    "\n",
    "    for result in results:\n",
    "        if result[\"DBCV Score\"] is not None:\n",
    "            result[\"DBCV Score\"] = scores.normalize_score(\n",
    "                result[\"DBCV Score\"], global_min, global_max\n",
    "            )\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    best_params_per_algorithm = {}\n",
    "\n",
    "    for algo_name in algorithms.keys():\n",
    "        algo_df = df[df[\"Algorithm\"] == algo_name]\n",
    "        if algo_df.empty:\n",
    "            print(f\"No data for {algo_name}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        best_params = {}\n",
    "        for score in score_columns:\n",
    "            if algo_df[score].isna().all():\n",
    "                print(f\"Warning: All values for {score} are NaN for {algo_name}. Skipping...\")\n",
    "                best_params[score] = {\"Parameters\": None, \"Score\": None}\n",
    "            else:\n",
    "                if score in [\"Davies-Bouldin\", \"Dunn Index\"]:\n",
    "                    best_row = algo_df.loc[algo_df[score].idxmin()]\n",
    "                else:\n",
    "                    best_row = algo_df.loc[algo_df[score].idxmax()]\n",
    "                best_params[score] = {\n",
    "                    \"Parameters\": best_row[\"Parameters\"],\n",
    "                    \"Score\": best_row[score]\n",
    "                }\n",
    "\n",
    "        best_params_per_algorithm[algo_name] = pd.DataFrame(best_params).T\n",
    "\n",
    "    for algo_name, best_params_df in best_params_per_algorithm.items():\n",
    "        print(f\"\\nBest Parameters for {algo_name}\")\n",
    "        display(best_params_df)\n",
    "        # Save best parameters as CSV\n",
    "        csv_filename = os.path.join(plot_dir, f\"best_params_{algo_name}_{'-'.join(map(str, year_range))}.csv\")\n",
    "        best_params_df.to_csv(csv_filename)\n",
    "\n",
    "    # Correlation matrix for each algorithm\n",
    "    for algo_name in algorithms.keys():\n",
    "        algo_df = df[df[\"Algorithm\"] == algo_name]\n",
    "        if not algo_df.empty:\n",
    "            correlation_matrix = algo_df[score_columns].corr(\"pearson\")\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(correlation_matrix, annot=True, cmap=\"Blues\", cbar=True, fmt='.2f', linewidths=0.5)\n",
    "            plt.title(f\"Correlation Matrix of Clustering Scores for {algo_name} Algorithm\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(plot_dir, f\"correlation_matrix_{algo_name}_{'-'.join(map(str, year_range))}.png\"))\n",
    "            plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
