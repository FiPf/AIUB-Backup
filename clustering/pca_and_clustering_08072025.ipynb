{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73bc2efb",
   "metadata": {},
   "source": [
    "# Plan after meeting with Nicola on 08.07.2025  \n",
    "1) Choose a set of data, 1 year, with the highest number of data  \n",
    "2) 7x7 scatter pairplot, no clustering  \n",
    "The 7 dimensions are: 6 orbital elements, mean motion   \n",
    "3) Apply PCA on that dataset:   \n",
    "-Create a ranking of the features based on pca_0807202 for that year  \n",
    "-Create a ranking of the features based on PCA for all years, compare different strategies for obtaining the rankings  \n",
    "4) Clustering in 4D using KMeans and create 4x4 pairplot  \n",
    "-Use i, a, e, Omega  \n",
    "-Use 4 top ranked features from PCA  \n",
    "5) Make clustering in 9D: 6 orbital elements, mean motion, magnitude, size  \n",
    "Make a pairplot, 6x6 with the 6 top ranked features, try again both ranking methods  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721f526a",
   "metadata": {},
   "source": [
    "This is no longer the plan, Alessandro changed it. New plan:   \n",
    "\n",
    "I had a meeting follow-up with Alessandro and, having a deeper look at the pairplot _k3, we cam up with the following tasks (which will replace the ones mentioned in the meeting):\n",
    "\n",
    "1) Fix a set of data (you can choose the same time interval you showed to me, 2005-2008) and do the same analysis for k > 3. This should be done up to a value of k from which you don't notice a significant difference with previous values of k;   \n",
    "2) For the same dataset, do the clustering (for different values of k) with all the features (6 orbital elements, mean motion, magnitude, diameter) and identify which of these features had the major contributions to the clusters identification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3652f3d3",
   "metadata": {},
   "source": [
    "# Step 1: Find the year with the most amount of data\n",
    "Result: 2006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969ef403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2002-2002 1750\n",
      "2003-2003 2487\n",
      "2004-2004 1649\n",
      "2005-2005 2151\n",
      "2006-2006 2592\n",
      "2007-2007 2157\n",
      "2008-2008 721\n",
      "2009-2009 2009\n",
      "2010-2010 799\n",
      "2011-2011 535\n",
      "2012-2012 217\n",
      "2013-2013 28\n",
      "2014-2014 86\n",
      "2015-2015 539\n",
      "2016-2016 674\n",
      "2017-2017 598\n",
      "2018-2018 831\n",
      "2019-2019 607\n",
      "2020-2020 1255\n",
      "2021-2021 749\n",
      "2022-2022 998\n",
      "2023-2023 1370\n",
      "The year with the most data is 2006-2006 with 2592 entries.\n"
     ]
    }
   ],
   "source": [
    "import cluster_data_pca_08072025\n",
    "from cluster_data_pca_08072025 import run_clustering, normalize_data, unnormalize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from DBSCAN import dbscan_clustering\n",
    "import cluster_plotter\n",
    "import high_dim_analysis\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scores\n",
    "\n",
    "running_ranges = cluster_data_pca_08072025.generate_running_year_ranges(2002, 2023, 1)\n",
    "bins = cluster_data_pca_08072025.bin_data_for_clustering(running_ranges, print_res=False)\n",
    "plot_dir = r\"Images\\\\oca_and_clustering_08072025\"\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "cluster_plotter.clear_directory(plot_dir)\n",
    "max_count = 0\n",
    "max_year = None\n",
    "\n",
    "max_count = 0\n",
    "max_year = None\n",
    "\n",
    "for cluster_data, year in bins:\n",
    "    count = len(cluster_data.ecc)\n",
    "    print(year, count)\n",
    "    if count > max_count:\n",
    "        max_count = count\n",
    "        max_year = year\n",
    "\n",
    "print(f\"The year with the most data is {max_year} with {max_count} entries.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfa795d",
   "metadata": {},
   "source": [
    "## 9D clustering and pairplot, plus contributions of features using PCA loadings\n",
    "Test on only one dataset, 2005 - 2008  \n",
    "for k = 3 to k = 12\n",
    "Compute the relevance/importance of the features (using PCA loadings), this is independent of k (PCA has nothing to do with clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99ec2c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running K-Means for Year Range: 2005-2008\n",
      "Runtime for k_means: 0.016351 seconds\n",
      "Runtime for k_means: 0.033661 seconds\n",
      "Runtime for k_means: 0.050035 seconds\n",
      "Runtime for k_means: 0.065413 seconds\n",
      "Runtime for k_means: 0.114370 seconds\n",
      "Runtime for k_means: 0.281544 seconds\n",
      "Runtime for k_means: 0.555828 seconds\n",
      "Runtime for k_means: 0.171348 seconds\n",
      "Runtime for k_means: 0.291775 seconds\n",
      "Runtime for k_means: 0.262312 seconds\n",
      "\n",
      "Feature ranking by PC1 loading contribution for 2005-2008:\n",
      "Semi major axis [km]: 21.57%\n",
      "Eccentricity e: 19.95%\n",
      "Mean Motion [rev/day]: 19.39%\n",
      "Inclination [°]: 14.94%\n",
      "Diameter [m]: 8.30%\n",
      "RAAN [°]: 5.43%\n",
      "Perigee [°]: 4.11%\n",
      "Magnitude [mag]: 3.46%\n",
      "True latitude [°]: 2.86%\n"
     ]
    }
   ],
   "source": [
    "import cluster_data_pca_08072025\n",
    "from cluster_data_pca_08072025 import run_clustering, normalize_data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from kmeans import k_means\n",
    "from clustering_utils_pca_08072025 import ClusterData\n",
    "import cluster_plotter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- Setup ---\n",
    "data_set = {\"2005-2008\": np.arange(2005, 2009)}\n",
    "binned_data = cluster_data_pca_08072025.bin_data_for_clustering(data_set, print_res=False)\n",
    "\n",
    "plot_dir = r\"Images\\tasks_10072025_kmeans_and_pairplots\"\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "cluster_plotter.clear_directory(plot_dir)\n",
    "\n",
    "# --- Loop over year ranges ---\n",
    "for cluster_data, year_range in binned_data:\n",
    "    print(f\"\\nRunning K-Means for Year Range: {year_range}\")\n",
    "\n",
    "    X = np.vstack([\n",
    "        cluster_data.ecc, cluster_data.sem_maj,\n",
    "        cluster_data.inc, cluster_data.raan,\n",
    "        cluster_data.perigee, cluster_data.true_lat,\n",
    "        cluster_data.mean_motion, cluster_data.mag_obj,\n",
    "        cluster_data.diameter\n",
    "    ]).T\n",
    "\n",
    "    feature_names = [\n",
    "        \"Eccentricity e\", \"Semi major axis [km]\", \"Inclination [°]\",\n",
    "        \"RAAN [°]\", \"Perigee [°]\", \"True latitude [°]\",\n",
    "        \"Mean Motion [rev/day]\", \"Magnitude [mag]\", \"Diameter [m]\"\n",
    "    ]\n",
    "\n",
    "    normalized_data, data_min, data_max = normalize_data(X)\n",
    "    k_values = [3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "\n",
    "    for k in k_values:\n",
    "        result, t, n_clusters, pts_per_cluster, _ = run_clustering(\n",
    "            k_means, f\"K-means (k={k})\",\n",
    "            normalized_data, data_min, data_max, k, init='kmeans++'\n",
    "        )\n",
    "        labels = result.labels\n",
    "\n",
    "        # --- Pairplot for this k ---\n",
    "        dfp = pd.DataFrame(X, columns=feature_names)\n",
    "        dfp['cluster'] = labels.astype(str)\n",
    "        pp = sns.pairplot(dfp, hue='cluster', diag_kind='kde',\n",
    "                          plot_kws={'alpha': 0.6, 's': 8}, corner = True)\n",
    "        pp.fig.suptitle(f\"K-Means Clusters {year_range} (k={k})\", y=1.02)\n",
    "        pp.savefig(os.path.join(plot_dir, f\"pairplot_{year_range}_k{k}.png\"))\n",
    "        plt.close(pp.fig)\n",
    "\n",
    "# --- PCA contribution ---\n",
    "# DOES NOT DEPEND ON K\n",
    "# Standardize raw X for PCA\n",
    "k = 3\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "loadings = pca.components_.T  # shape: (n_features, n_components)\n",
    "\n",
    "# PC1 only\n",
    "pc1_loadings = np.abs(loadings[:, 0])\n",
    "pc1_contrib = pc1_loadings / pc1_loadings.sum() * 100\n",
    "\n",
    "sorted_idx = np.argsort(pc1_contrib)[::-1]\n",
    "sorted_features = np.array(feature_names)[sorted_idx]\n",
    "sorted_importance = pc1_contrib[sorted_idx]\n",
    "\n",
    "# Plot PC1 loadings\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.barh(sorted_features, sorted_importance)\n",
    "plt.xlabel(\"PC1 Loading Contribution [%]\")\n",
    "plt.title(f\"PCA Feature Contribution (PC1) for {year_range} (independent of k)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(plot_dir, f\"pca_feature_importance_{year_range}.png\"))\n",
    "plt.close()\n",
    "\n",
    "print(f\"\\nFeature ranking by PC1 loading contribution for {year_range}:\")\n",
    "for feat, imp in zip(sorted_features, sorted_importance):\n",
    "    print(f\"{feat}: {imp:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b16c7c",
   "metadata": {},
   "source": [
    "# Task from Alessandro: test dbscan with different tuning parameters with dbcv score\n",
    "In 2D and 3D, tuning was easier, as we could use eye inspection to verify. In higher dimensions, we need a score. The only score that works in DBCV score, which is very computationally expensive (despite being written in Rust). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11a829f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DBSCAN: eps=0.08, min_samples=4\n",
      "Runtime for dbscan_clustering: 0.177648 seconds\n",
      "DBCV Score: -0.366\n",
      "Running DBSCAN: eps=0.08, min_samples=7\n",
      "Runtime for dbscan_clustering: 0.438085 seconds\n",
      "DBCV Score: -0.570\n",
      "Running DBSCAN: eps=0.08, min_samples=10\n",
      "Runtime for dbscan_clustering: 0.411318 seconds\n",
      "DBCV Score: -0.636\n",
      "Running DBSCAN: eps=0.08, min_samples=20\n",
      "Runtime for dbscan_clustering: 0.398099 seconds\n",
      "DBCV Score: -0.686\n",
      "Running DBSCAN: eps=0.1, min_samples=4\n",
      "Runtime for dbscan_clustering: 0.471691 seconds\n",
      "DBCV Score: -0.438\n",
      "Running DBSCAN: eps=0.1, min_samples=7\n",
      "Runtime for dbscan_clustering: 0.178404 seconds\n",
      "DBCV Score: -0.647\n",
      "Running DBSCAN: eps=0.1, min_samples=10\n",
      "Runtime for dbscan_clustering: 0.166043 seconds\n",
      "DBCV Score: -0.674\n",
      "Running DBSCAN: eps=0.1, min_samples=20\n",
      "Runtime for dbscan_clustering: 0.203169 seconds\n",
      "DBCV Score: -0.704\n",
      "Running DBSCAN: eps=0.15, min_samples=4\n",
      "Runtime for dbscan_clustering: 0.298523 seconds\n",
      "DBCV Score: -0.505\n",
      "Running DBSCAN: eps=0.15, min_samples=7\n",
      "Runtime for dbscan_clustering: 0.277018 seconds\n",
      "DBCV Score: -0.672\n",
      "Running DBSCAN: eps=0.15, min_samples=10\n",
      "Runtime for dbscan_clustering: 0.271490 seconds\n",
      "DBCV Score: -0.653\n",
      "Running DBSCAN: eps=0.15, min_samples=20\n",
      "Runtime for dbscan_clustering: 0.281847 seconds\n",
      "DBCV Score: -0.870\n",
      "Running DBSCAN: eps=0.2, min_samples=4\n",
      "Runtime for dbscan_clustering: 0.393124 seconds\n",
      "DBCV Score: -0.533\n",
      "Running DBSCAN: eps=0.2, min_samples=7\n",
      "Runtime for dbscan_clustering: 0.352329 seconds\n",
      "DBCV Score: -0.571\n",
      "Running DBSCAN: eps=0.2, min_samples=10\n",
      "Runtime for dbscan_clustering: 0.349469 seconds\n",
      "DBCV Score: -0.584\n",
      "Running DBSCAN: eps=0.2, min_samples=20\n",
      "Runtime for dbscan_clustering: 0.349690 seconds\n",
      "DBCV Score: -0.824\n",
      "Running DBSCAN: eps=0.25, min_samples=4\n",
      "Runtime for dbscan_clustering: 0.528212 seconds\n",
      "DBCV Score: -0.526\n",
      "Running DBSCAN: eps=0.25, min_samples=7\n",
      "Runtime for dbscan_clustering: 0.453963 seconds\n",
      "DBCV Score: -0.581\n",
      "Running DBSCAN: eps=0.25, min_samples=10\n",
      "Runtime for dbscan_clustering: 0.454439 seconds\n",
      "DBCV Score: -0.703\n",
      "Running DBSCAN: eps=0.25, min_samples=20\n",
      "Runtime for dbscan_clustering: 0.432618 seconds\n",
      "DBCV Score: -0.749\n",
      "\n",
      "Best parameters: eps=0.08, min_samples=4, score=-0.366\n",
      "\n",
      "Stored plot for eps = 0.08, min_samples = 4.\n",
      "Stored plot for eps = 0.08, min_samples = 7.\n",
      "Stored plot for eps = 0.08, min_samples = 10.\n",
      "Stored plot for eps = 0.08, min_samples = 20.\n",
      "Stored plot for eps = 0.1, min_samples = 4.\n",
      "Stored plot for eps = 0.1, min_samples = 7.\n",
      "Stored plot for eps = 0.1, min_samples = 10.\n",
      "Stored plot for eps = 0.1, min_samples = 20.\n",
      "Stored plot for eps = 0.15, min_samples = 4.\n",
      "Stored plot for eps = 0.15, min_samples = 7.\n",
      "Stored plot for eps = 0.15, min_samples = 10.\n",
      "Stored plot for eps = 0.15, min_samples = 20.\n",
      "Stored plot for eps = 0.2, min_samples = 4.\n",
      "Stored plot for eps = 0.2, min_samples = 7.\n",
      "Stored plot for eps = 0.2, min_samples = 10.\n",
      "Stored plot for eps = 0.2, min_samples = 20.\n",
      "Stored plot for eps = 0.25, min_samples = 4.\n",
      "Stored plot for eps = 0.25, min_samples = 7.\n",
      "Stored plot for eps = 0.25, min_samples = 10.\n",
      "Stored plot for eps = 0.25, min_samples = 20.\n",
      "All pairplots saved to: Images/dbscan_9d_tuning_dbcv_score\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "import cluster_data_pca_08072025 as cdp\n",
    "from cluster_data_pca_08072025 import run_clustering, normalize_data\n",
    "from DBSCAN import dbscan_clustering\n",
    "import cluster_plotter\n",
    "from scores import DBCV_score_rust\n",
    "\n",
    "year_range = \"2005-2008\"\n",
    "data_set = {year_range: np.arange(2005, 2009)}\n",
    "plot_dir = r\"Images/dbscan_9d_tuning_dbcv_score_\"\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "cluster_plotter.clear_directory(plot_dir)\n",
    "\n",
    "binned = cdp.bin_data_for_clustering(data_set, print_res=False)\n",
    "data_obj, _ = binned[0]\n",
    "\n",
    "X = np.vstack([\n",
    "    data_obj.ecc,\n",
    "    data_obj.sem_maj,\n",
    "    data_obj.inc,\n",
    "    data_obj.raan,\n",
    "    data_obj.perigee,\n",
    "    data_obj.true_lat,\n",
    "    data_obj.mean_motion,\n",
    "    data_obj.mag_obj,\n",
    "    data_obj.diameter\n",
    "]).T\n",
    "\n",
    "norm_X, X_min, X_max = normalize_data(X)\n",
    "\n",
    "# Start from an eps tuned in 2D (eps_2d) and scale to 9D: eps_9d = eps_2d * sqrt(9/2)\n",
    "eps_2d_list = [0.08, 0.1, 0.15, 0.2, 0.25]  # example 2D eps candidates\n",
    "eps_factor = 1\n",
    "eps_vals = [round(e * eps_factor, 3) for e in eps_2d_list]\n",
    "\n",
    "n_dim = X.shape[1]\n",
    "ms_vals = [4, 7, 10, 20]\n",
    "\n",
    "# --- Run DBSCAN for each combination and record scores ---\n",
    "results = []\n",
    "for eps in eps_vals:\n",
    "    for ms in ms_vals:\n",
    "        print(f\"Running DBSCAN: eps={eps}, min_samples={ms}\")\n",
    "        clustering, duration, n_clusters, pts_per_cluster, _ = run_clustering(\n",
    "            dbscan_clustering, \"DBSCAN\", norm_X, X_min, X_max,\n",
    "            eps=eps, min_samples=ms\n",
    "        )\n",
    "        labels = clustering.labels\n",
    "        score = DBCV_score_rust(clustering)\n",
    "        print(f\"DBCV Score: {score:.3f}\")\n",
    "        results.append({\n",
    "            \"eps\": eps,\n",
    "            \"min_samples\": ms,\n",
    "            \"n_clusters\": n_clusters,\n",
    "            \"DBCV_score\": score,\n",
    "            \"labels\": labels\n",
    "        })\n",
    "\n",
    "# --- Generate pairplots and find best params ---\n",
    "best = max(results, key=lambda r: r[\"DBCV_score\"])\n",
    "best_eps, best_ms = best[\"eps\"], best[\"min_samples\"]\n",
    "print(f\"\\nBest parameters: eps={best_eps}, min_samples={best_ms}, score={best['DBCV_score']:.3f}\\n\")\n",
    "\n",
    "for res in results:\n",
    "    eps, ms, lbls = res[\"eps\"], res[\"min_samples\"], res[\"labels\"]\n",
    "    df = pd.DataFrame(X, columns=[\n",
    "        \"ecc\", \"a\", \"i\", \"raan\", \"perigee\", \"lat\", \"mn\", \"mag\", \"diam\"\n",
    "    ])\n",
    "    df['cluster'] = lbls.astype(str)\n",
    "    pp = sns.pairplot(df, hue='cluster', diag_kind='kde', plot_kws={'alpha':0.6, 's':8}, corner = True)\n",
    "    pp.fig.suptitle(f\"DBSCAN 9D Clusters {year_range} (eps={eps}, ms={ms})\", y=1.02)\n",
    "    fname = f\"pairplot_{year_range}_eps{eps}_ms{ms}.png\"\n",
    "    pp.savefig(os.path.join(plot_dir, fname))\n",
    "    plt.close(pp.fig)\n",
    "    print(f\"Stored plot for eps = {eps}, min_samples = {ms}.\")\n",
    "\n",
    "print(\"All pairplots saved to:\", plot_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
